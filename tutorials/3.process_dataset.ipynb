{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and read the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset from hdf5 file and show the data struction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "def get_all_hdf5_files(directory):\n",
    "    \"\"\"\n",
    "    Get all HDF5 files in a directory and its subdirectories.\n",
    "    \"\"\"\n",
    "    hdf5_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.hdf5'):\n",
    "                hdf5_files.append(os.path.join(root, file))\n",
    "    return hdf5_files\n",
    "\n",
    "def print_structure(group, indent=0):\n",
    "    \"\"\"\n",
    "    Print the hierarchical structure of an HDF5 group.\n",
    "    \"\"\"\n",
    "    for key in group.keys():\n",
    "        item = group[key]\n",
    "        print(\" \" * indent + key)\n",
    "        if isinstance(item, h5py.Group):\n",
    "            print_structure(item, indent + 4)\n",
    "        elif isinstance(item, h5py.Dataset):\n",
    "            print(\" \" * indent, item.shape, item.dtype)\n",
    "            if key == \"entities\" or key == \"target_entity\" or key == \"instruction\":\n",
    "                decoded = [x.decode('utf-8') for x in item]\n",
    "                print(\" \" * indent, decoded)\n",
    "        \n",
    "        elif isinstance(item, h5py.Datatype):\n",
    "            print(\" \" * (indent + 4) + \"Datatype\", json.loads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/data/310_jiarui/datafactory/get_coffee/data_52.hdf5', '/mnt/data/310_jiarui/datafactory/get_coffee/data_50.hdf5', '/mnt/data/310_jiarui/datafactory/get_coffee/data_0.hdf5']\n"
     ]
    }
   ],
   "source": [
    "dataset_root = \"/mnt/data/310_jiarui/datafactory/get_coffee\" \n",
    "hdf5_files = get_all_hdf5_files(dataset_root)\n",
    "print(hdf5_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "    2025-07-09 17:16:11\n",
      "        instruction\n",
      "         (1,) |S23\n",
      "         ['Get me a cup of coffee.']\n",
      "        meta_info\n",
      "            entities\n",
      "             (4,) |S14\n",
      "             ['table', 'coffee_machine', 'mug_seen', 'bottom']\n",
      "            episode_config\n",
      "             () |S1460\n",
      "            target_entity\n",
      "             (1,) |S8\n",
      "             ['mug_seen']\n",
      "        observation\n",
      "            depth\n",
      "             (211, 6, 480, 480) float32\n",
      "            ee_state\n",
      "             (211, 8) float32\n",
      "            point_cloud_colors\n",
      "             (211, 30330, 3) float32\n",
      "            point_cloud_points\n",
      "             (211, 30330, 3) float32\n",
      "            q_acceleration\n",
      "             (211, 7) float32\n",
      "            q_state\n",
      "             (211, 7) float32\n",
      "            q_velocity\n",
      "             (211, 7) float32\n",
      "            rgb\n",
      "             (211, 6, 480, 480, 3) uint8\n",
      "            robot_mask\n",
      "             (211, 6, 480, 480) float32\n",
      "        trajectory\n",
      "         (211, 8) float32\n"
     ]
    }
   ],
   "source": [
    "example_file = random.choice(hdf5_files)\n",
    "with h5py.File(example_file, 'r') as f:\n",
    "    print_structure(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert to tf dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the baseline methods, OpenVLA and Octo are trained using datasets in the TFDS format. In this repository, we have modified the tfds dataset builder to convert the HDF5 source format of VLABench into TFDS format. We offer [singlethread tfds builder](../VLABench/utils/rlds_builder.py) and [multithread tfds builder](../VLABench/utils/multithread_rlds_builder.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the dataset, run\n",
    "```sh\n",
    "python scripts/convert_to_rlds.py --save_dir /your/vlabench/dataset/root --task {target_task}\n",
    "```\n",
    "to create the tfds builder in the dataset directory.\n",
    "Then, run\n",
    "```sh\n",
    "cd /your/vlabench/dataset/root/target_task\n",
    "tfds build --overwrite\n",
    "```\n",
    "to generate rlds format dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend the readers to refer to https://github.com/kpertsch/rlds_dataset_builder for further guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert to Lerobot dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the baseline methods, $\\pi_0$ uses lerobot format dataset for finetining. We offer a script to convert hdf5 format dataset into lerobot format, similar to Libero dataset used in $\\pi_0$.\n",
    "\n",
    "Run \n",
    "```sh\n",
    "python scripts/convert_to_lerobot.py --dataset-name xxx --dataset-path /your/path/to/hdf5 --max-files 500 --task-list task1 task2 ... task n\n",
    "```\n",
    "to create a multi-task lerobot dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3562125674.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python scripts/convert_to_lerobot.py --dataset-name get_coff_simple_100 --dataset-path /mnt/data/310_jiarui/datafactory --max-files 500 --task-list get_coffee\u001b[0m\n\u001b[0m                                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "python scripts/convert_to_lerobot.py --dataset-name get_coff_simple_100 --dataset-path /mnt/data/310_jiarui/datafactory --max-files 500 --task-list get_coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python scripts/convert_to_lerobot.py --dataset-name set_study_table --dataset-path /mnt/data/310_jiarui/datafactory/fix --max-files 500 --task-list set_study_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlabench_tube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
